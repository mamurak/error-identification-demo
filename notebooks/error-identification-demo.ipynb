{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31531a5e-2e7b-42e2-9314-0b884d7cde16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install dotenv==0.9.9 llama_stack==0.2.6 requests==2.32.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import uuid\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from llama_stack_client import Agent, LlamaStackClient, RAGDocument\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from requests import post\n",
    "from termcolor import cprint\n",
    "\n",
    "from src.loki import query_loki_logs\n",
    "from src.utils import step_printer\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb6808-719a-48d1-8f67-b59877f6d39e",
   "metadata": {},
   "source": [
    "### Kick off customer onboarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3a4f4-13c1-465f-b25b-b7e93e628376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def id_generator(size=6, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))\n",
    "\n",
    "\n",
    "def onboard_customer():\n",
    "    customer_onboarding_service_url = 'http://customer-onboarding-service-customer-onboarding.apps.cluster-srb7z.srb7z.sandbox2014.opentlc.com/'\n",
    "    payload = {\n",
    "        'customerId': f'CUST-{id_generator(chars=string.digits)}',\n",
    "        'fullName': id_generator(chars=string.ascii_letters),\n",
    "        'nationalId': id_generator(),\n",
    "        'birthDate': id_generator(chars=string.digits),\n",
    "    }\n",
    "    post(customer_onboarding_service_url, json=payload)\n",
    "\n",
    "\n",
    "n_customers = 10\n",
    "#for i in range(n_customers):\n",
    "#    onboard_customer()\n",
    "\n",
    "print(f'submitted {n_customers} new customers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf70ace-b704-4379-aa88-3c793cc4f959",
   "metadata": {},
   "source": [
    "### RAG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08281062-f222-4443-a7bc-0f6166aab36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "client = LlamaStackClient(base_url=base_url)\n",
    "\n",
    "print(\"Connected to Llama Stack server\")\n",
    "\n",
    "model_id = \"llama32-3b\"\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = 100000  # int(os.getenv(\"MAX_TOKENS\", 512))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "# For this demo, we are using Milvus Lite, which is our preferred solution. Any other Vector DB supported by Llama Stack can be used.\n",
    "\n",
    "# RAG vector DB settings\n",
    "VECTOR_DB_EMBEDDING_MODEL = os.getenv(\"VDB_EMBEDDING\")\n",
    "VECTOR_DB_EMBEDDING_DIMENSION = int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384))\n",
    "VECTOR_DB_CHUNK_SIZE = int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512))\n",
    "VECTOR_DB_PROVIDER_ID = os.getenv(\"VDB_PROVIDER\")\n",
    "\n",
    "# Unique DB ID for session\n",
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"False\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=os.getenv(\"VDB_EMBEDDING\"),\n",
    "    embedding_dimension=int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384)),\n",
    "    provider_id=os.getenv(\"VDB_PROVIDER\"),\n",
    ")\n",
    "\n",
    "# ingest the documents into the newly created document collection\n",
    "urls = [\n",
    "    (\"https://raw.githubusercontent.com/mamurak/error-identification-demo/refs/heads/main/source_docs/onboarding-application.pdf\", \"application/pdf\"),\n",
    "]\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=url,\n",
    "        mime_type=url_type,\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, (url, url_type) in enumerate(urls)\n",
    "]\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512)),\n",
    ")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2cedaf-522b-4251-886a-d8aa7b9fcd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of registered tools and extract their toolgroup IDs\n",
    "registered_tools = client.tools.list()\n",
    "registered_toolgroups = [tool.toolgroup_id for tool in registered_tools]\n",
    "\n",
    "if \"builtin::rag\" not in registered_toolgroups:\n",
    "    client.toolgroups.register(\n",
    "        toolgroup_id=\"builtin::rag\",\n",
    "        provider_id=\"milvus\"\n",
    "    )\n",
    "\n",
    "# Log the current toolgroups registered\n",
    "print(f\"Your Llama Stack server is already registered with the following tool groups: {set(registered_toolgroups)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3136e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt = \"\"\"\n",
    "    You are a helpful assistant. You must use the knowledge search tool to answer user questions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462d89b-b641-4615-9c78-86e2f5c0484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "builtin_rag = dict(\n",
    "    name=\"builtin::rag\",\n",
    "    args={\"vector_db_ids\": [vector_db_id]},\n",
    ")\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client=client,\n",
    "    model=model_id,\n",
    "    instructions=rag_prompt,\n",
    "    tools=[builtin_rag],\n",
    "    sampling_params={\"max_tokens\": 100000},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac5521-0439-4caa-9c28-8e39e2b5b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    \"\"\"What is the AmlValidationService?\"\"\"\n",
    "]\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=rag_agent.create_session(f\"rag-session_{uuid.uuid4()}\"),\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f49fe6d-0d08-49bd-b5cc-2195ef21e9aa",
   "metadata": {},
   "source": [
    "### Loki tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d16971d-4259-43ca-a718-8667ab21df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are a helpful assistant that retrieves error logs from Kubernetes containers using Loki.\n",
    "\n",
    "When users ask for error logs:\n",
    "1. Use the query_loki_logs tool to retrieve actual logs from containers from the past 1 hour\n",
    "2. Extract error and failure messages found within the retrieved logs\n",
    "3. Present the error messages in a readable format with timestamps and parsed messages\n",
    "4. If no logs are found, suggest checking container/namespace names\n",
    "\n",
    "The logs are returned in a parsed format showing timestamp and the actual log message content.\n",
    "Always use the tool when log data is requested rather than giving general explanations.\n",
    "\"\"\"\n",
    "\n",
    "loki_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=instructions,\n",
    "    tools=[query_loki_logs],\n",
    "    sampling_params={\"max_tokens\": 100000},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3563e2-3a1e-4ff1-ae48-30deb9d1dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    \"\"\"Get error logs from container customer-validation-service in namespace customer-onboarding\"\"\"\n",
    "]\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = loki_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=loki_agent.create_session(f\"loki-session_{uuid.uuid4()}\"),\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e19486e-778b-4beb-87de-e4bf7bd7e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are a helpful assistant that retrieves logs from Kubernetes containers using Loki to identify the processing status of individual customers.\n",
    "\n",
    "When users ask for the onboarding status of a customer with a given customer ID:\n",
    "1. Use the query_loki_logs tool to retrieve the logs from the 'customer-validation-service' container in namespace 'customer-onboarding' from the last 1 hour\n",
    "2. Look for all logged messages associated with the given customer ID\n",
    "3. Present the messages in a readable format with timestamps and parsed messages\n",
    "4. If no logs are found, suggest checking container/namespace names\n",
    "\n",
    "The logs are returned in a parsed format showing timestamp and the actual log message content.\n",
    "Always use the tool when log data is requested rather than giving general explanations.\n",
    "\"\"\"\n",
    "\n",
    "loki_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=instructions,\n",
    "    tools=[query_loki_logs],\n",
    "    sampling_params={\"max_tokens\": 100000},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752e41e-4027-4e4c-b2f5-71cc15ac1509",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    \"\"\"What is the onboarding status of customer CUST-695437?\"\"\"\n",
    "]\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = loki_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=loki_agent.create_session(f\"loki-session_{uuid.uuid4()}\"),\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c986dcf-ee5f-4164-972a-fad4e97c1ad8",
   "metadata": {},
   "source": [
    "### RAG + Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec60fe76-a837-4f69-882f-2b5c5039df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are a helpful assistant. You have access to a number of tools.\n",
    "Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\n",
    "\"\"\"\n",
    "\n",
    "full_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=instructions,\n",
    "    tools=[query_loki_logs, builtin_rag],\n",
    "    sampling_params={\"max_tokens\": 100000},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218a1849-fa75-4a51-9302-7f2de4b375fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    \"Retrieve the logs from the 'aml-validation-service' container in namespace 'customer-onboarding' from the last 1 hour\",\n",
    "    \"Within the retrieved logs find any error messages associated with customer ID CUST-695437\",\n",
    "    \"Use the knowledge search tool to look up additional details about this error message\",\n",
    "    \"Report your findings and provide a summary based on your search\",\n",
    "]\n",
    "session_id = full_agent.create_session(session_name=\"full\")\n",
    "for i, prompt in enumerate(user_prompts):\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = full_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e072d-d57d-407e-8196-531c51954d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
