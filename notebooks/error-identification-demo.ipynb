{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ecd807-fb1c-41eb-a948-365e57396d90",
   "metadata": {},
   "source": [
    "# Level 6: Agents, MCP and RAG \n",
    "\n",
    "This notebook is an extension of the [Level 5 Agentic & MCP notebook](./Level5_agents_and_mcp.ipynb) with the addition of RAG.\n",
    "This tutorial is for developers who are already familiar with [agentic RAG workflows](./Level4_RAG_agent.ipynb). This tutorial will highlight a couple of slightly more advanced use cases for agents where a single tool call is insufficient to complete the required task. Here we will rely on both agentic RAG and MCP server to expand our agents capabilities.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers the following steps:\n",
    "1. Review OpenShift logs for a failing pod.\n",
    "2. Categorize the pod and summarize its error.\n",
    "3. Search available troubleshooting documentations for ideas on how to resolve the error.\n",
    "4. Send a Slack message to the ops team with a brief summary of the error and next steps to take.\n",
    "\n",
    "### MCP Tools:\n",
    "\n",
    "#### OpenShift MCP Server\n",
    "Throughout this notebook we will be relying on the [kuberenetes-mcp-server](https://github.com/manusa/kubernetes-mcp-server) by [manusa](https://github.com/manusa) to interact with our OpenShift cluster. Please see installation instructions below if you do not already have this deployed in your environment. \n",
    "\n",
    "* [OpenShift MCP installation instructions](../../../kubernetes/mcp-servers/openshift-mcp/README.md)\n",
    "\n",
    "#### Slack MCP Server\n",
    "We will also be using the [Slack MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/slack) in this notebook. Please see installation instructions below if you do not already have this deployed in your environment. \n",
    "\n",
    "* [Slack MCP installation instructions](../../../kubernetes/mcp-servers/slack-mcp/README.md)\n",
    "\n",
    "### Pre-Requisites\n",
    "\n",
    "Before starting, ensure you have the following:\n",
    "- A running Llama Stack server\n",
    "- A running Slack MCP server. Refer to our [documentation](https://github.com/opendatahub-io/llama-stack-demos/tree/main/kubernetes/mcp-servers/slack-mcp) on how you can set this up on your OpenShift cluster\n",
    "- Access to an OpeShift cluster with a deployment of the [OpenShift MCP server](../../../kubernetes/mcp-servers/openshift-mcp) (see the [deployment manifests](https://github.com/opendatahub-io/llama-stack-on-ocp/tree/main/kubernetes/mcp-servers/openshift-mcp) for assistance with this).\n",
    "\n",
    "## Setting Up this Notebook\n",
    "We will initialize our environment as described in detail in our [\\\"Getting Started\\\" notebook](./Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31531a5e-2e7b-42e2-9314-0b884d7cde16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install dotenv llama_stack==0.2.6 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import time\n",
    "import urllib.parse\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from llama_stack_client import Agent, LlamaStackClient, RAGDocument\n",
    "from llama_stack_client.lib.agents.client_tool import client_tool\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
    "from llama_stack_client.lib.agents.react.tool_parser import ReActOutput\n",
    "import requests\n",
    "from termcolor import cprint\n",
    "\n",
    "from src.utils import step_printer\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb6808-719a-48d1-8f67-b59877f6d39e",
   "metadata": {},
   "source": [
    "### Kick off customer onboarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3a4f4-13c1-465f-b25b-b7e93e628376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "from requests import post\n",
    "\n",
    "\n",
    "def id_generator(size=6, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))\n",
    "\n",
    "\n",
    "def onboard_customer():\n",
    "    customer_onboarding_service_url = 'http://customer-onboarding-service-customer-onboarding.apps.cluster-srb7z.srb7z.sandbox2014.opentlc.com/'\n",
    "    payload = {\n",
    "        'customerId': f'CUST-{id_generator(chars=string.digits)}',\n",
    "        'fullName': id_generator(chars=string.ascii_letters),\n",
    "        'nationalId': id_generator(),\n",
    "        'birthDate': id_generator(chars=string.digits),\n",
    "    }\n",
    "    post(customer_onboarding_service_url, json=payload)\n",
    "\n",
    "\n",
    "n_customers = 10\n",
    "for i in range(n_customers):\n",
    "    onboard_customer()\n",
    "\n",
    "print(f'submitted {n_customers} new customers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf70ace-b704-4379-aa88-3c793cc4f959",
   "metadata": {},
   "source": [
    "### RAG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08281062-f222-4443-a7bc-0f6166aab36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "client = LlamaStackClient(base_url=base_url)\n",
    "\n",
    "print(\"Connected to Llama Stack server\")\n",
    "\n",
    "model_id = \"llama32-3b\"\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = 100000  # int(os.getenv(\"MAX_TOKENS\", 512))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "# For this demo, we are using Milvus Lite, which is our preferred solution. Any other Vector DB supported by Llama Stack can be used.\n",
    "\n",
    "# RAG vector DB settings\n",
    "VECTOR_DB_EMBEDDING_MODEL = os.getenv(\"VDB_EMBEDDING\")\n",
    "VECTOR_DB_EMBEDDING_DIMENSION = int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384))\n",
    "VECTOR_DB_CHUNK_SIZE = int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512))\n",
    "VECTOR_DB_PROVIDER_ID = os.getenv(\"VDB_PROVIDER\")\n",
    "\n",
    "# Unique DB ID for session\n",
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"False\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=os.getenv(\"VDB_EMBEDDING\"),\n",
    "    embedding_dimension=int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384)),\n",
    "    provider_id=os.getenv(\"VDB_PROVIDER\"),\n",
    ")\n",
    "\n",
    "# ingest the documents into the newly created document collection\n",
    "urls = [\n",
    "    (\"https://raw.githubusercontent.com/mamurak/error-identification-demo/refs/heads/main/source_docs/onboarding-application.pdf\", \"application/pdf\"),\n",
    "]\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=url,\n",
    "        mime_type=url_type,\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, (url, url_type) in enumerate(urls)\n",
    "]\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512)),\n",
    ")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2cedaf-522b-4251-886a-d8aa7b9fcd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocp_mcp_url = os.getenv(\"REMOTE_OCP_MCP_URL\")  # Optional: enter your MCP server url here\n",
    "\n",
    "# Get list of registered tools and extract their toolgroup IDs\n",
    "registered_tools = client.tools.list()\n",
    "registered_toolgroups = [tool.toolgroup_id for tool in registered_tools]\n",
    "\n",
    "if \"builtin::rag\" not in registered_toolgroups:\n",
    "    client.toolgroups.register(\n",
    "        toolgroup_id=\"builtin::rag\",\n",
    "        provider_id=\"milvus\"\n",
    "    )\n",
    "\n",
    "if \"mcp::openshift\" not in registered_toolgroups:\n",
    "    client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::openshift\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\": ocp_mcp_url},\n",
    "    )\n",
    "# Log the current toolgroups registered\n",
    "print(f\"Your Llama Stack server is already registered with the following tool groups: {set(registered_toolgroups)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3136e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt = \"\"\"\n",
    "    You are a helpful assistant. You must use the knowledge search tool to answer user questions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462d89b-b641-4615-9c78-86e2f5c0484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "builtin_rag = dict(\n",
    "    name=\"builtin::rag\",\n",
    "    args={\"vector_db_ids\": [vector_db_id]},\n",
    ")\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client=client,\n",
    "    model=model_id,\n",
    "    instructions=rag_prompt,\n",
    "    tools=[builtin_rag],\n",
    "    sampling_params={\"max_tokens\": 100000},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac5521-0439-4caa-9c28-8e39e2b5b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    \"\"\"What is the AmlValidationService?\"\"\"\n",
    "]\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=rag_agent.create_session(f\"rag-session_{uuid.uuid4()}\"),\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f49fe6d-0d08-49bd-b5cc-2195ef21e9aa",
   "metadata": {},
   "source": [
    "### Loki tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98803fef-6277-4402-860c-d7919f3ec04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log_message(raw_log: str) -> str:\n",
    "    \"\"\"\n",
    "    Parse log message similar to jq logic:\n",
    "    Try to parse as JSON and extract msg/message/log fields, \n",
    "    otherwise return the raw log.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to parse as JSON\n",
    "        log_json = json.loads(raw_log)\n",
    "        \n",
    "        # Extract the message field (similar to jq: .msg // .message // .log // .)\n",
    "        if isinstance(log_json, dict):\n",
    "            # Priority order: msg -> message -> log -> entire object\n",
    "            message = (\n",
    "                log_json.get('msg') or \n",
    "                log_json.get('message') or \n",
    "                log_json.get('log') or \n",
    "                str(log_json)\n",
    "            )\n",
    "            return str(message)\n",
    "        else:\n",
    "            # If it's not a dict, return as string\n",
    "            return str(log_json)\n",
    "            \n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # If it's not JSON, return the raw log\n",
    "        return raw_log\n",
    "\n",
    "\n",
    "@client_tool\n",
    "def query_loki_logs(namespace: str, container_name: str, hours: str = '1') -> str:\n",
    "    \"\"\"Query logs from a namespace and container in Loki with enhanced JSON parsing.\n",
    "    :param namespace: Kubernetes namespace name\n",
    "    :param container_name: Container name to query logs from\n",
    "    :param hours: Age of oldest logs to filter in hours\n",
    "    :returns: Log messages\n",
    "    \"\"\"\n",
    "    logger.info(f'loki function called with: namespace={namespace}, container_name={container_name}, hours={hours}')\n",
    "    try:\n",
    "        print(f\"🔍 Querying Loki logs for namespace: {namespace}, container: {container_name}, hours: {hours}\")\n",
    "        \n",
    "        # Updated token (use your actual token)\n",
    "        token = os.getenv(\"TOKEN\")\n",
    "        \n",
    "        # Calculate time range in ISO format\n",
    "        now = datetime.now(timezone.utc)\n",
    "        start_time = now - timedelta(hours=int(hours))\n",
    "        \n",
    "        # Format timestamps as ISO 8601 strings\n",
    "        start_iso = start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        end_iso = now.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        \n",
    "        # Base URL for Loki\n",
    "        base_url = os.getenv(\"LOKI_BASE_URL\")\n",
    "        url = f\"{base_url}/api/logs/v1/application/loki/api/v1/query_range\"\n",
    "        \n",
    "        # Use the working LogQL query pattern - search by container name in pod names\n",
    "        # Since container names are often part of pod names, we'll use regex matching\n",
    "        logql_query = f'{{kubernetes_namespace_name=\"{namespace}\",kubernetes_pod_name=~\".*{container_name}.*\"}}'\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {token}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Accept\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        params = {\n",
    "            \"query\": logql_query,\n",
    "            \"start\": start_iso,\n",
    "            \"end\": end_iso,\n",
    "            \"limit\": 5000\n",
    "        }\n",
    "        \n",
    "        print(f\"📡 Making request to: {url}\")\n",
    "        print(f\"📋 Query: {logql_query}\")\n",
    "        print(f\"🕒 Time range: {start_iso} to {end_iso}\")\n",
    "        \n",
    "        res = requests.get(\n",
    "            url, \n",
    "            params=params, \n",
    "            headers=headers, \n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        print(f\"📊 Response Status: {res.status_code}\")\n",
    "        \n",
    "        if res.status_code == 200:\n",
    "            try:\n",
    "                response_data = res.json()\n",
    "                \n",
    "                # Check if we got data\n",
    "                if response_data.get(\"status\") == \"success\":\n",
    "                    parsed_logs = []\n",
    "                    results = response_data.get(\"data\", {}).get(\"result\", [])\n",
    "                    \n",
    "                    for result in results:\n",
    "                        for entry in result.get(\"values\", []):\n",
    "                            if len(entry) >= 2:\n",
    "                                # entry[0] is timestamp (nanoseconds), entry[1] is log message\n",
    "                                timestamp_ns = entry[0]\n",
    "                                raw_log = entry[1]\n",
    "                                \n",
    "                                # Convert timestamp from nanoseconds to human-readable format\n",
    "                                try:\n",
    "                                    timestamp_seconds = int(timestamp_ns) / 1_000_000_000\n",
    "                                    formatted_timestamp = datetime.fromtimestamp(timestamp_seconds, tz=timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "                                except (ValueError, TypeError):\n",
    "                                    formatted_timestamp = str(timestamp_ns)\n",
    "                                \n",
    "                                # Parse the log message (similar to jq logic)\n",
    "                                parsed_message = parse_log_message(raw_log)\n",
    "                                \n",
    "                                # Format output: timestamp + parsed message\n",
    "                                log_line = f\"{formatted_timestamp} {parsed_message}\"\n",
    "                                parsed_logs.append(log_line)\n",
    "                                print(log_line)\n",
    "                    \n",
    "                    if parsed_logs:\n",
    "                        log_output = \"\\n\".join(parsed_logs)\n",
    "                        print(f\"✅ Successfully retrieved and parsed {len(parsed_logs)} log entries\")\n",
    "                        return f\"Found {len(parsed_logs)} parsed log entries for container '{container_name}' in namespace '{namespace}':\\n\\n{log_output}\"\n",
    "                    else:\n",
    "                        print(\"📭 No log entries found in results\")\n",
    "                        return f\"❌ No logs found for container '{container_name}' in namespace '{namespace}' for the last {hours} hour(s)\"\n",
    "                else:\n",
    "                    error_msg = f\"❌ Loki returned error status: {response_data.get('error', 'Unknown error')}\"\n",
    "                    print(error_msg)\n",
    "                    return error_msg\n",
    "                    \n",
    "            except json.JSONDecodeError as je:\n",
    "                error_msg = f\"❌ JSON decode error: {je}\"\n",
    "                print(error_msg)\n",
    "                return error_msg\n",
    "                \n",
    "        else:\n",
    "            error_msg = f\"❌ HTTP Error {res.status_code}: {res.text}\"\n",
    "            print(error_msg)\n",
    "            return error_msg\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        error_msg = f\"❌ Request failed: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "    except Exception as e:\n",
    "        error_msg = f\"❌ Unexpected error querying Loki: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d16971d-4259-43ca-a718-8667ab21df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are a helpful assistant that retrieves error logs from Kubernetes containers using Loki.\n",
    "\n",
    "When users ask for error logs:\n",
    "1. Use the query_loki_logs tool to retrieve actual logs from containers\n",
    "2. Extract error and failure messages found within the retrieved logs\n",
    "3. Present the error messages in a readable format with timestamps and parsed messages\n",
    "4. If no logs are found, suggest checking container/namespace names\n",
    "\n",
    "The logs are returned in a parsed format showing timestamp and the actual log message content.\n",
    "Always use the tool when log data is requested rather than giving general explanations.\n",
    "\"\"\"\n",
    "\n",
    "loki_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=instructions,\n",
    "    tools=[query_loki_logs],\n",
    "    sampling_params={\"max_tokens\": 100000},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3563e2-3a1e-4ff1-ae48-30deb9d1dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    \"\"\"Get error logs from container customer-validation-service in namespace customer-onboarding\"\"\"\n",
    "]\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = loki_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=loki_agent.create_session(f\"loki-session_{uuid.uuid4()}\"),\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e19486e-778b-4beb-87de-e4bf7bd7e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are a helpful assistant that retrieves logs from Kubernetes containers using Loki to identify the processing status of individual customers.\n",
    "\n",
    "When users ask for the onboarding status of a customer with a given customer ID:\n",
    "1. Use the query_loki_logs tool to retrieve the logs from the 'customer-validation-service' container in namespace 'customer-onboarding' from the last 100 hours\n",
    "2. Look for all logged messages associated with the given customer ID\n",
    "3. Present the messages in a readable format with timestamps and parsed messages\n",
    "4. If no logs are found, suggest checking container/namespace names\n",
    "\n",
    "The logs are returned in a parsed format showing timestamp and the actual log message content.\n",
    "Always use the tool when log data is requested rather than giving general explanations.\n",
    "\"\"\"\n",
    "\n",
    "loki_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=instructions,\n",
    "    tools=[query_loki_logs],\n",
    "    sampling_params={\"max_tokens\": 100000},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752e41e-4027-4e4c-b2f5-71cc15ac1509",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    \"\"\"What is the onboarding status of customer CUST-45231?\"\"\"\n",
    "]\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = loki_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=loki_agent.create_session(f\"loki-session_{uuid.uuid4()}\"),\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c986dcf-ee5f-4164-972a-fad4e97c1ad8",
   "metadata": {},
   "source": [
    "### RAG + Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec60fe76-a837-4f69-882f-2b5c5039df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are a helpful assistant. You have access to a number of tools.\n",
    "Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "full_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=instructions,\n",
    "    tools=[query_loki_logs, builtin_rag],\n",
    "    sampling_params={\"max_tokens\": 100000},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218a1849-fa75-4a51-9302-7f2de4b375fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    \"Retrieve the logs from the 'aml-validation-service' container in namespace 'customer-onboarding' from the last 100 hours\",\n",
    "    \"Within the retrieved logs find any error messages associated with customer ID CUST-45231\",\n",
    "    \"Use the knowledge search tool to look up additional details about this error message\",\n",
    "    \"Report your findings and provide a summary based on your search\",\n",
    "]\n",
    "session_id = full_agent.create_session(session_name=\"full\")\n",
    "for i, prompt in enumerate(user_prompts):\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = full_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e072d-d57d-407e-8196-531c51954d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
