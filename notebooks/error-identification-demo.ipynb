{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ecd807-fb1c-41eb-a948-365e57396d90",
   "metadata": {},
   "source": [
    "# Level 6: Agents, MCP and RAG \n",
    "\n",
    "This notebook is an extension of the [Level 5 Agentic & MCP notebook](./Level5_agents_and_mcp.ipynb) with the addition of RAG.\n",
    "This tutorial is for developers who are already familiar with [agentic RAG workflows](./Level4_RAG_agent.ipynb). This tutorial will highlight a couple of slightly more advanced use cases for agents where a single tool call is insufficient to complete the required task. Here we will rely on both agentic RAG and MCP server to expand our agents capabilities.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers the following steps:\n",
    "1. Review OpenShift logs for a failing pod.\n",
    "2. Categorize the pod and summarize its error.\n",
    "3. Search available troubleshooting documentations for ideas on how to resolve the error.\n",
    "4. Send a Slack message to the ops team with a brief summary of the error and next steps to take.\n",
    "\n",
    "### MCP Tools:\n",
    "\n",
    "#### OpenShift MCP Server\n",
    "Throughout this notebook we will be relying on the [kuberenetes-mcp-server](https://github.com/manusa/kubernetes-mcp-server) by [manusa](https://github.com/manusa) to interact with our OpenShift cluster. Please see installation instructions below if you do not already have this deployed in your environment. \n",
    "\n",
    "* [OpenShift MCP installation instructions](../../../kubernetes/mcp-servers/openshift-mcp/README.md)\n",
    "\n",
    "#### Slack MCP Server\n",
    "We will also be using the [Slack MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/slack) in this notebook. Please see installation instructions below if you do not already have this deployed in your environment. \n",
    "\n",
    "* [Slack MCP installation instructions](../../../kubernetes/mcp-servers/slack-mcp/README.md)\n",
    "\n",
    "### Pre-Requisites\n",
    "\n",
    "Before starting, ensure you have the following:\n",
    "- A running Llama Stack server\n",
    "- A running Slack MCP server. Refer to our [documentation](https://github.com/opendatahub-io/llama-stack-demos/tree/main/kubernetes/mcp-servers/slack-mcp) on how you can set this up on your OpenShift cluster\n",
    "- Access to an OpeShift cluster with a deployment of the [OpenShift MCP server](../../../kubernetes/mcp-servers/openshift-mcp) (see the [deployment manifests](https://github.com/opendatahub-io/llama-stack-on-ocp/tree/main/kubernetes/mcp-servers/openshift-mcp) for assistance with this).\n",
    "\n",
    "## Setting Up this Notebook\n",
    "We will initialize our environment as described in detail in our [\\\"Getting Started\\\" notebook](./Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  \n",
    "import uuid\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from llama_stack_client import Agent, LlamaStackClient, RAGDocument\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
    "from llama_stack_client.lib.agents.react.tool_parser import ReActOutput\n",
    "from termcolor import cprint\n",
    "\n",
    "from src.utils import step_printer\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08281062-f222-4443-a7bc-0f6166aab36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "client = LlamaStackClient(base_url=base_url)\n",
    "\n",
    "print(\"Connected to Llama Stack server\")\n",
    "\n",
    "model_id = \"granite32-8b\"\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 512))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "# For this demo, we are using Milvus Lite, which is our preferred solution. Any other Vector DB supported by Llama Stack can be used.\n",
    "\n",
    "# RAG vector DB settings\n",
    "VECTOR_DB_EMBEDDING_MODEL = os.getenv(\"VDB_EMBEDDING\")\n",
    "VECTOR_DB_EMBEDDING_DIMENSION = int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384))\n",
    "VECTOR_DB_CHUNK_SIZE = int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512))\n",
    "VECTOR_DB_PROVIDER_ID = os.getenv(\"VDB_PROVIDER\")\n",
    "\n",
    "# Unique DB ID for session\n",
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"False\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=os.getenv(\"VDB_EMBEDDING\"),\n",
    "    embedding_dimension=int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384)),\n",
    "    provider_id=os.getenv(\"VDB_PROVIDER\"),\n",
    ")\n",
    "\n",
    "# ingest the documents into the newly created document collection\n",
    "urls = [\n",
    "    (\"https://raw.githubusercontent.com/mamurak/error-identification-demo/refs/heads/main/source_docs/onboarding-application.pdf\", \"application/pdf\"),\n",
    "]\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=url,\n",
    "        mime_type=url_type,\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, (url, url_type) in enumerate(urls)\n",
    "]\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512)),\n",
    ")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66044170",
   "metadata": {},
   "source": [
    "### Validate tools are available in our Llama Stack instance\n",
    "\n",
    "We will be using the [OpenShift MCP Server](https://github.com/manusa/kubernetes-mcp-server) and the [Slack MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/slack) for performing this task. If you haven't already, you can follow the instructions [here](https://github.com/opendatahub-io/llama-stack-demos/tree/main/kubernetes/mcp-servers/slack-mcp#setting-up-on-ocp) to install the Slack MCP server and the instructions [here](https://github.com/opendatahub-io/llama-stack-demos/tree/main/kubernetes/mcp-servers/openshift-mcp#steps-for-deploying-the-openshift-mcp-server-on-openshift) to install the OpenShift MCP server. Once we confirm that the OpenShift and Slack MCP servers are running and configured to your Llama Stack server, we can then move on to defining an agent to help with our task.\n",
    "\n",
    "#### Registering tools on Llama Stack\n",
    "When an instance of llama stack is redeployed your tools need to re-registered. Also if a tool is already registered with a llama stack instance, if you try to register one with the same `toolgroup_id`, llama stack will throw you an error.\n",
    "\n",
    "For this reason it is recommended to include some code to validate your tools and toolgroups. This is where the `mcp_url` comes into play. The following code will check that the `builtin::rag`,`mcp::openshift`  and `mcp::slack` tools are registered as tools, but if any mcp tool is not listed there, it will attempt to register it using the mcp url.\n",
    "\n",
    "If you are running the MCP server from source, the default value for this is: `http://localhost:8000/sse`.\n",
    "\n",
    "If you are running the MCP server from a container, the default value for this is: `http://host.containers.internal:8000/sse`.\n",
    "\n",
    "Make sure to pass the corresponding MCP URL for the server you are trying to register/validate tools for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2cedaf-522b-4251-886a-d8aa7b9fcd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocp_mcp_url = os.getenv(\"REMOTE_OCP_MCP_URL\")  # Optional: enter your MCP server url here\n",
    "\n",
    "# Get list of registered tools and extract their toolgroup IDs\n",
    "registered_tools = client.tools.list()\n",
    "registered_toolgroups = [tool.toolgroup_id for tool in registered_tools]\n",
    "\n",
    "if \"builtin::rag\" not in registered_toolgroups:\n",
    "    client.toolgroups.register(\n",
    "        toolgroup_id=\"builtin::rag\",\n",
    "        provider_id=\"milvus\"\n",
    "    )\n",
    "\n",
    "if \"mcp::openshift\" not in registered_toolgroups:\n",
    "    client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::openshift\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\": ocp_mcp_url},\n",
    "    )\n",
    "# Log the current toolgroups registered\n",
    "print(f\"Your Llama Stack server is already registered with the following tool groups: {set(registered_toolgroups)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf70ace-b704-4379-aa88-3c793cc4f959",
   "metadata": {},
   "source": [
    "### System Prompt for LLM model\n",
    "\n",
    "**Note:** If you have multiple models configured with your Llama Stack server, you can choose which one to run your queries against. When switching to a different model, you may need to adjust the system prompt to align with that model’s expected behavior. Many models provide recommended system prompts for optimal and reliable outputs these are typically documented on their respective websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3136e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prompt = \"\"\"\n",
    "    You are a helpful assistant.\n",
    "    You're an expert on troubleshooting financial applications running on OpenShift.\n",
    "    You're talking to a business user with little technical background,\n",
    "    so you do your best to simplify technical details.\n",
    "    You have access to a number of tools.\n",
    "    You must use the knowledge search tool to validate your responses.\n",
    "    Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6c84d",
   "metadata": {},
   "source": [
    "## Defining our Agent - ReAct\n",
    "\n",
    "Now that we've shown that we can successfully accomplish this multi-step multi-tool task using prompt chaining, let's see if we can give our agent a bit more autonomy to perform the same task but with a single prompt instead of a chian. To do this, we will instantiate a ReAct agent (which is included in the llama stack python client by default). The ReAct agent is a variant of the simple agent but with the ability to loop through \"Reason then Act\" iterations, thinking through the problem and then using tools until it determines that it's task has been completed successfully.\n",
    "\n",
    "Unlike prompt chaining which follows fixed steps, ReAct dynamically breaks down tasks and adapts its approach based on the results of each step. This makes it more flexible and capable of handling complex, real-world queries effectively.\n",
    "\n",
    "Below you will see the slight differences in the agent definition and the prompt used to accomplish our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462d89b-b641-4615-9c78-86e2f5c0484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "builtin_rag = dict(\n",
    "    name=\"builtin::rag\",\n",
    "    args={\"vector_db_ids\": [vector_db_id]},\n",
    ")\n",
    "\n",
    "agent = ReActAgent(\n",
    "    client=client,\n",
    "    model=model_id,\n",
    "    instructions=model_prompt,\n",
    "    tools=[\"mcp::openshift\", builtin_rag],\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": ReActOutput.model_json_schema(),\n",
    "    },\n",
    "    sampling_params={\"max_tokens\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac5521-0439-4caa-9c28-8e39e2b5b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    \"\"\"View the logs for pod customer-onboarding-service-544c966d99-z6j9w in the customer-onboarding OpenShift namespace.\n",
    "    Categorize it as normal or error, and provide a summary of the steps to take in just 1-2 sentences.\"\"\"\n",
    "]\n",
    "\n",
    "session_id = agent.create_session(\"web-session\")\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a081b-c076-4ba9-b91a-5b3a90865c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    \"\"\"View the logs for pod customer-onboarding-service-544c966d99-z6j9w in the customer-onboarding OpenShift namespace.\n",
    "    Check whether you can find the processing status for CUST-102938.\n",
    "    If you find any processing failures for this ID, provide a root cause analysis.\"\"\"\n",
    "]\n",
    "\n",
    "session_id = agent.create_session(\"web-session\")\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892eb8f-70c1-4caf-98df-71ba6fb5ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    \"\"\"Can you list the pods in the customer-onboarding OpenShift namespace?\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "session_id = agent.create_session(\"web-session\")\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da3c72-f51c-4e29-a4e0-58c7ebb0396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What is the AmlValidationService? Please search our knowledge base for an answer.\",\n",
    "]\n",
    "\n",
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "    \n",
    "    # create a new turn with a new session ID for each prompt\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=agent.create_session(f\"rag-session_{uuid.uuid4()}\"),\n",
    "        stream=stream,\n",
    "    )\n",
    "    \n",
    "    # print the response, including tool calls output\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db68ad-96c1-4231-9aed-588f902332d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    \"\"\"\n",
    "    View the logs for pod aml-validation-service-7cd65c4bdf-d78fw in the customer-onboarding OpenShift namespace.\n",
    "    Check whether you can find the processing status for CUST-45231.\n",
    "    If you find a processing error, search for solutions on this error and provide a summary.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "session_id = agent.create_session(\"web-session\")\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cae097-b323-4dce-8d80-78e51a3415c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
