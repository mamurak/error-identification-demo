{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up this Notebook\n",
    "We will start with a few imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîß COMPREHENSIVE RAG SETUP WITH DISCOVERY\n",
      "============================================================\n",
      "üîç Discovering Vector DB Providers...\n",
      "========================================\n",
      "üìã Vector DBs response: {\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"identifier\": \"test_vector_db_6a2ba4eb-cece-4e0b-ad1e-d0cb07bfc9f1\",\n",
      "      \"provider_resource_id\": \"test_vector_db_6a2ba4eb-cece-4e0b-ad1e-d0cb07bfc9f1\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"test_vector_db_e95d55b1-f8a4-4b0c-97ef-8b3dee20f796\",\n",
      "      \"provider_resource_id\": \"test_vector_db_e95d55b1-f8a4-4b0c-97ef-8b3dee20f796\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"test_vector_db_d5554528-59de-4b69-bb98-cd7b90efbaec\",\n",
      "      \"provider_resource_id\": \"test_vector_db_d5554528-59de-4b69-bb98-cd7b90efbaec\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"test_vector_db_16dbb0e3-d452-4bfe-b752-6c19814a7b1d\",\n",
      "      \"provider_resource_id\": \"test_vector_db_16dbb0e3-d452-4bfe-b752-6c19814a7b1d\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"test_vector_db_ee9565a4-9a2e-4e66-ae12-5190b42236cc\",\n",
      "      \"provider_resource_id\": \"test_vector_db_ee9565a4-9a2e-4e66-ae12-5190b42236cc\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"test_vector_db_2e80f095-624a-4941-9496-2b8382bbd21b\",\n",
      "      \"provider_resource_id\": \"test_vector_db_2e80f095-624a-4941-9496-2b8382bbd21b\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"test_vector_db_843a70e5-9e28-433a-a931-dbe656af4f75\",\n",
      "      \"provider_resource_id\": \"test_vector_db_843a70e5-9e28-433a-a931-dbe656af4f75\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"test_vector_db_bb439f98-e434-487b-87f6-148942305124\",\n",
      "      \"provider_resource_id\": \"test_vector_db_bb439f98-e434-487b-87f6-148942305124\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"test_vector_db_e843f976-0dd3-4c14-ae6c-62aaf964d06a\",\n",
      "      \"provider_resource_id\": \"test_vector_db_e843f976-0dd3-4c14-ae6c-62aaf964d06a\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"openshift_docs_d772b914-06c5-4179-96cd-1933c63cca28\",\n",
      "      \"provider_resource_id\": \"openshift_docs_d772b914-06c5-4179-96cd-1933c63cca28\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"openshift_docs_a1514e56-6204-4107-b95c-4ecd5e20f3a0\",\n",
      "      \"provider_resource_id\": \"openshift_docs_a1514e56-6204-4107-b95c-4ecd5e20f3a0\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"openshift_docs_ec1fbc3d-1b93-46d5-be9f-4ebae17f41f6\",\n",
      "      \"provider_resource_id\": \"openshift_docs_ec1fbc3d-1b93-46d5-be9f-4ebae17f41f6\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"openshift_docs_5fd16471-cc88-4f4c-974a-fe04def7411f\",\n",
      "      \"provider_resource_id\": \"openshift_docs_5fd16471-cc88-4f4c-974a-fe04def7411f\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"openshift_docs_f4ad6010-f550-44e1-abe0-210d4dc22d31\",\n",
      "      \"provider_resource_id\": \"openshift_docs_f4ad6010-f550-44e1-abe0-210d4dc22d31\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"openshift_docs_7205b4ab-516f-4621-8823-1c3390477e78\",\n",
      "      \"provider_resource_id\": \"openshift_docs_7205b4ab-516f-4621-8823-1c3390477e78\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"openshift_docs_0193317f-7b69-4427-aec2-eb8f764eb9e6\",\n",
      "      \"provider_resource_id\": \"openshift_docs_0193317f-7b69-4427-aec2-eb8f764eb9e6\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"openshift_docs_1120f4a8-f68f-471e-a356-eb093b2757ba\",\n",
      "      \"provider_resource_id\": \"openshift_docs_1120f4a8-f68f-471e-a356-eb093b2757ba\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"openshift_docs_23e3b470-6e74-4686-b43f-9674dfab83e1\",\n",
      "      \"provider_resource_id\": \"openshift_docs_23e3b470-6e74-4686-b43f-9674dfab83e1\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"openshift_docs_7ac79bf3-6053-4b85-93b0-3e903279fdd3\",\n",
      "      \"provider_resource_id\": \"openshift_docs_7ac79bf3-6053-4b85-93b0-3e903279fdd3\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"test_vector_db_6f15d6ff-c2a1-46df-8f08-789629abc07e\",\n",
      "      \"provider_resource_id\": \"test_vector_db_6f15d6ff-c2a1-46df-8f08-789629abc07e\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"test_vector_db_4eb73612-a9e8-46c7-9d6c-7e3fb99dc3c7\",\n",
      "      \"provider_resource_id\": \"test_vector_db_4eb73612-a9e8-46c7-9d6c-7e3fb99dc3c7\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"test_vector_db_6fa248e6-f128-43f6-8b70-d1fe7a74c012\",\n",
      "      \"provider_resource_id\": \"test_vector_db_6fa248e6-f128-43f6-8b70-d1fe7a74c012\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    },\n",
      "    {\n",
      "      \"identifier\": \"test_vector_db_5d9c2f40-5646-413f-b432-7ce3f4d3cd6b\",\n",
      "      \"provider_resource_id\": \"test_vector_db_5d9c2f40-5646-413f-b432-7ce3f4d3cd6b\",\n",
      "      \"provider_id\": \"milvus\",\n",
      "      \"type\": \"vector_db\",\n",
      "      \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "      \"embedding_dimension\": 384\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "üìã Existing vector DBs: [{'identifier': 'test_vector_db_6a2ba4eb-cece-4e0b-ad1e-d0cb07bfc9f1', 'provider_resource_id': 'test_vector_db_6a2ba4eb-cece-4e0b-ad1e-d0cb07bfc9f1', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'test_vector_db_e95d55b1-f8a4-4b0c-97ef-8b3dee20f796', 'provider_resource_id': 'test_vector_db_e95d55b1-f8a4-4b0c-97ef-8b3dee20f796', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'test_vector_db_d5554528-59de-4b69-bb98-cd7b90efbaec', 'provider_resource_id': 'test_vector_db_d5554528-59de-4b69-bb98-cd7b90efbaec', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'test_vector_db_16dbb0e3-d452-4bfe-b752-6c19814a7b1d', 'provider_resource_id': 'test_vector_db_16dbb0e3-d452-4bfe-b752-6c19814a7b1d', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'test_vector_db_ee9565a4-9a2e-4e66-ae12-5190b42236cc', 'provider_resource_id': 'test_vector_db_ee9565a4-9a2e-4e66-ae12-5190b42236cc', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'test_vector_db_2e80f095-624a-4941-9496-2b8382bbd21b', 'provider_resource_id': 'test_vector_db_2e80f095-624a-4941-9496-2b8382bbd21b', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'test_vector_db_843a70e5-9e28-433a-a931-dbe656af4f75', 'provider_resource_id': 'test_vector_db_843a70e5-9e28-433a-a931-dbe656af4f75', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'test_vector_db_bb439f98-e434-487b-87f6-148942305124', 'provider_resource_id': 'test_vector_db_bb439f98-e434-487b-87f6-148942305124', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'test_vector_db_e843f976-0dd3-4c14-ae6c-62aaf964d06a', 'provider_resource_id': 'test_vector_db_e843f976-0dd3-4c14-ae6c-62aaf964d06a', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'openshift_docs_d772b914-06c5-4179-96cd-1933c63cca28', 'provider_resource_id': 'openshift_docs_d772b914-06c5-4179-96cd-1933c63cca28', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'openshift_docs_a1514e56-6204-4107-b95c-4ecd5e20f3a0', 'provider_resource_id': 'openshift_docs_a1514e56-6204-4107-b95c-4ecd5e20f3a0', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'openshift_docs_ec1fbc3d-1b93-46d5-be9f-4ebae17f41f6', 'provider_resource_id': 'openshift_docs_ec1fbc3d-1b93-46d5-be9f-4ebae17f41f6', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'openshift_docs_5fd16471-cc88-4f4c-974a-fe04def7411f', 'provider_resource_id': 'openshift_docs_5fd16471-cc88-4f4c-974a-fe04def7411f', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'openshift_docs_f4ad6010-f550-44e1-abe0-210d4dc22d31', 'provider_resource_id': 'openshift_docs_f4ad6010-f550-44e1-abe0-210d4dc22d31', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'openshift_docs_7205b4ab-516f-4621-8823-1c3390477e78', 'provider_resource_id': 'openshift_docs_7205b4ab-516f-4621-8823-1c3390477e78', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'openshift_docs_0193317f-7b69-4427-aec2-eb8f764eb9e6', 'provider_resource_id': 'openshift_docs_0193317f-7b69-4427-aec2-eb8f764eb9e6', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'openshift_docs_1120f4a8-f68f-471e-a356-eb093b2757ba', 'provider_resource_id': 'openshift_docs_1120f4a8-f68f-471e-a356-eb093b2757ba', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'openshift_docs_23e3b470-6e74-4686-b43f-9674dfab83e1', 'provider_resource_id': 'openshift_docs_23e3b470-6e74-4686-b43f-9674dfab83e1', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'openshift_docs_7ac79bf3-6053-4b85-93b0-3e903279fdd3', 'provider_resource_id': 'openshift_docs_7ac79bf3-6053-4b85-93b0-3e903279fdd3', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'test_vector_db_6f15d6ff-c2a1-46df-8f08-789629abc07e', 'provider_resource_id': 'test_vector_db_6f15d6ff-c2a1-46df-8f08-789629abc07e', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'test_vector_db_4eb73612-a9e8-46c7-9d6c-7e3fb99dc3c7', 'provider_resource_id': 'test_vector_db_4eb73612-a9e8-46c7-9d6c-7e3fb99dc3c7', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'test_vector_db_6fa248e6-f128-43f6-8b70-d1fe7a74c012', 'provider_resource_id': 'test_vector_db_6fa248e6-f128-43f6-8b70-d1fe7a74c012', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}, {'identifier': 'test_vector_db_5d9c2f40-5646-413f-b432-7ce3f4d3cd6b', 'provider_resource_id': 'test_vector_db_5d9c2f40-5646-413f-b432-7ce3f4d3cd6b', 'provider_id': 'milvus', 'type': 'vector_db', 'embedding_model': 'all-MiniLM-L6-v2', 'embedding_dimension': 384}]\n",
      "üîÑ Testing provider: chroma\n",
      "‚úÖ Provider chroma available (but embedding model not found)\n",
      "üîÑ Testing provider: qdrant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Provider qdrant available (but embedding model not found)\n",
      "üîÑ Testing provider: weaviate\n",
      "‚úÖ Provider weaviate available (but embedding model not found)\n",
      "üîÑ Testing provider: pinecone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Provider pinecone available (but embedding model not found)\n",
      "üîÑ Testing provider: elasticsearch\n",
      "‚úÖ Provider elasticsearch available (but embedding model not found)\n",
      "üîÑ Testing provider: opensearch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Provider opensearch available (but embedding model not found)\n",
      "üîÑ Testing provider: milvus\n",
      "‚úÖ Provider milvus available (but embedding model not found)\n",
      "üîÑ Testing provider: pgvector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: GET http://llamastack-server:8321/v1/embeddings \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: GET http://llamastack-server:8321/v1/embedding-models \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: GET http://llamastack-server:8321/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Provider pgvector available (but embedding model not found)\n",
      "\n",
      "‚úÖ Working providers: ['chroma', 'qdrant', 'weaviate', 'pinecone', 'elasticsearch', 'opensearch', 'milvus', 'pgvector']\n",
      "\n",
      "üîç Discovering Embedding Models...\n",
      "========================================\n",
      "‚úÖ /v1/models works:\n",
      "üìã Embedding models found: [{'identifier': 'all-MiniLM-L6-v2', 'provider_resource_id': 'all-MiniLM-L6-v2', 'provider_id': 'sentence-transformers', 'type': 'model', 'metadata': {'embedding_dimension': 384}, 'model_type': 'embedding'}]\n",
      "‚úÖ Embedding models: [{'identifier': 'all-MiniLM-L6-v2', 'provider_resource_id': 'all-MiniLM-L6-v2', 'provider_id': 'sentence-transformers', 'type': 'model', 'metadata': {'embedding_dimension': 384}, 'model_type': 'embedding'}]\n",
      "\n",
      "üß™ Testing Vector DB Creation...\n",
      "========================================\n",
      "üîÑ Testing: provider=chroma, model={'identifier': 'all-MiniLM-L6-v2', 'provider_resource_id': 'all-MiniLM-L6-v2', 'provider_id': 'sentence-transformers', 'type': 'model', 'metadata': {'embedding_dimension': 384}, 'model_type': 'embedding'}\n",
      "‚ùå Failed with chroma/all-MiniLM-L6-v2: Error code: 400 - {'error': {'detail': {'errors': [{'loc': ['body', 'embedding_model'], 'msg': 'Field required', 'type': 'missing'}]}}}\n",
      "üîÑ Testing LLM as embedding: provider=chroma, model=llama32-3b\n",
      "‚ùå LLM embedding failed chroma/llama32-3b: Error code: 400 - {'detail': 'Invalid value: Model llama32-3b is not an embedding model'}\n",
      "üîÑ Testing LLM as embedding: provider=qdrant, model=llama32-3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå LLM embedding failed qdrant/llama32-3b: Error code: 400 - {'detail': 'Invalid value: Model llama32-3b is not an embedding model'}\n",
      "üîÑ Testing LLM as embedding: provider=weaviate, model=llama32-3b\n",
      "‚ùå LLM embedding failed weaviate/llama32-3b: Error code: 400 - {'detail': 'Invalid value: Model llama32-3b is not an embedding model'}\n",
      "üîÑ Testing LLM as embedding: provider=pinecone, model=llama32-3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå LLM embedding failed pinecone/llama32-3b: Error code: 400 - {'detail': 'Invalid value: Model llama32-3b is not an embedding model'}\n",
      "üîÑ Testing LLM as embedding: provider=elasticsearch, model=llama32-3b\n",
      "‚ùå LLM embedding failed elasticsearch/llama32-3b: Error code: 400 - {'detail': 'Invalid value: Model llama32-3b is not an embedding model'}\n",
      "üîÑ Testing LLM as embedding: provider=opensearch, model=llama32-3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå LLM embedding failed opensearch/llama32-3b: Error code: 400 - {'detail': 'Invalid value: Model llama32-3b is not an embedding model'}\n",
      "üîÑ Testing LLM as embedding: provider=milvus, model=llama32-3b\n",
      "‚ùå LLM embedding failed milvus/llama32-3b: Error code: 400 - {'detail': 'Invalid value: Model llama32-3b is not an embedding model'}\n",
      "üîÑ Testing LLM as embedding: provider=pgvector, model=llama32-3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-server:8321/v1/vector-dbs \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå LLM embedding failed pgvector/llama32-3b: Error code: 400 - {'detail': 'Invalid value: Model llama32-3b is not an embedding model'}\n",
      "‚úÖ Vector DB: None\n",
      "\n",
      "ü§ñ Creating RAG Agent with Vector DB: None\n",
      "========================================\n",
      "‚ùå No working vector DB available\n",
      "‚úÖ RAG Agent: None\n"
     ]
    }
   ],
   "source": [
    "# RAG Integration Discovery and Fix\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "from llama_stack_client import LlamaStackClient, RAGDocument\n",
    "\n",
    "def discover_vector_db_providers(client):\n",
    "    \"\"\"Discover what vector DB providers are actually available.\"\"\"\n",
    "    print(\"üîç Discovering Vector DB Providers...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Try to get existing vector databases to see what's configured\n",
    "    try:\n",
    "        response = client._client.get(f\"{client._base_url}/v1/vector-dbs\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"üìã Vector DBs response: {json.dumps(data, indent=2)}\")\n",
    "            \n",
    "            # Look for provider information\n",
    "            if 'providers' in data:\n",
    "                providers = data['providers']\n",
    "                print(f\"‚úÖ Available providers: {providers}\")\n",
    "                return providers\n",
    "            elif 'data' in data:\n",
    "                print(f\"üìã Existing vector DBs: {data['data']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not get vector DB info: {e}\")\n",
    "    \n",
    "    # Try common providers\n",
    "    common_providers = [\n",
    "        \"chroma\", \"qdrant\", \"weaviate\", \"pinecone\", \n",
    "        \"elasticsearch\", \"opensearch\", \"milvus\", \"pgvector\"\n",
    "    ]\n",
    "    \n",
    "    working_providers = []\n",
    "    for provider in common_providers:\n",
    "        try:\n",
    "            print(f\"üîÑ Testing provider: {provider}\")\n",
    "            test_db_id = f\"test_{provider}_{int(time.time())}\"\n",
    "            \n",
    "            # Try to create a test vector DB with this provider\n",
    "            client.vector_dbs.register(\n",
    "                vector_db_id=test_db_id,\n",
    "                embedding_model=\"test-model\",  # This will likely fail, but we want to see the error\n",
    "                embedding_dimension=384,\n",
    "                provider_id=provider\n",
    "            )\n",
    "            working_providers.append(provider)\n",
    "            print(f\"‚úÖ Provider {provider} works!\")\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            if \"not found\" in error_msg.lower() and \"provider\" in error_msg.lower():\n",
    "                print(f\"‚ùå Provider {provider} not available\")\n",
    "            elif \"model\" in error_msg.lower() and \"not found\" in error_msg.lower():\n",
    "                print(f\"‚úÖ Provider {provider} available (but embedding model not found)\")\n",
    "                working_providers.append(provider)\n",
    "            else:\n",
    "                print(f\"‚ùå Provider {provider} error: {error_msg[:100]}\")\n",
    "    \n",
    "    return working_providers\n",
    "\n",
    "\n",
    "def discover_embedding_models(client):\n",
    "    \"\"\"Try to discover what embedding models are available.\"\"\"\n",
    "    print(\"\\nüîç Discovering Embedding Models...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Check if there's an embeddings endpoint\n",
    "    endpoints_to_check = [\n",
    "        \"/v1/embeddings\",\n",
    "        \"/v1/embedding-models\", \n",
    "        \"/v1/models\",\n",
    "        \"/embeddings\"\n",
    "    ]\n",
    "    \n",
    "    for endpoint in endpoints_to_check:\n",
    "        try:\n",
    "            response = client._client.get(f\"{client._base_url}{endpoint}\")\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                print(f\"‚úÖ {endpoint} works:\")\n",
    "                \n",
    "                # Look for embedding models\n",
    "                if 'data' in data:\n",
    "                    models = data['data']\n",
    "                    embedding_models = [\n",
    "                        model for model in models \n",
    "                        if 'embedding' in str(model).lower() or \n",
    "                        model.get('model_type') == 'embedding'\n",
    "                    ]\n",
    "                    if embedding_models:\n",
    "                        print(f\"üìã Embedding models found: {embedding_models}\")\n",
    "                        return embedding_models\n",
    "                \n",
    "                print(f\"üìã Response: {json.dumps(data, indent=2)[:300]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {endpoint}: {str(e)[:50]}\")\n",
    "    \n",
    "    # Try common embedding model names with the models we know exist\n",
    "    known_models = [\"llama32-3b\"]\n",
    "    print(f\"\\nüîÑ Testing if known models support embeddings...\")\n",
    "    \n",
    "    for model in known_models:\n",
    "        print(f\"  Testing: {model}\")\n",
    "        # We'll test this when we try to create the vector DB\n",
    "    \n",
    "    return []\n",
    "\n",
    "\n",
    "def test_vector_db_with_discovered_info(client, providers, embedding_models):\n",
    "    \"\"\"Test vector DB creation with discovered providers and models.\"\"\"\n",
    "    print(f\"\\nüß™ Testing Vector DB Creation...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    vector_db_id = f\"working_vector_db_{int(time.time())}\"\n",
    "    \n",
    "    # If we found working providers and models, use them\n",
    "    if providers and embedding_models:\n",
    "        for provider in providers[:1]:  # Test first working provider\n",
    "            for model in embedding_models[:1]:  # Test first embedding model\n",
    "                try:\n",
    "                    print(f\"üîÑ Testing: provider={provider}, model={model}\")\n",
    "                    \n",
    "                    model_name = model if isinstance(model, str) else model.get('identifier', str(model))\n",
    "                    \n",
    "                    client.vector_dbs.register(\n",
    "                        vector_db_id=f\"{vector_db_id}_{provider}_{model_name}\",\n",
    "                        embedding_model=os.getenv(\"VDB_EMBEDDING\"),\n",
    "                        embedding_dimension=384,  # Start with common dimension\n",
    "                        provider_id=os.getenv(\"VDB_PROVIDER\")\n",
    "                    )\n",
    "                    \n",
    "                    final_db_id = f\"{vector_db_id}_{provider}_{model_name}\"\n",
    "                    print(f\"‚úÖ Vector DB created successfully: {final_db_id}\")\n",
    "                    return final_db_id\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Failed with {provider}/{model_name}: {e}\")\n",
    "    \n",
    "    # If no embedding models found, try using the main LLM models\n",
    "    if providers:\n",
    "        known_models = [\"llama32-3b\"]\n",
    "        for provider in providers:\n",
    "            for model in known_models:\n",
    "                try:\n",
    "                    print(f\"üîÑ Testing LLM as embedding: provider={provider}, model={model}\")\n",
    "                    \n",
    "                    client.vector_dbs.register(\n",
    "                        vector_db_id=f\"{vector_db_id}_{provider}_{model}\",\n",
    "                        embedding_model=model,\n",
    "                        embedding_dimension=4096,  # Larger dimension for LLM\n",
    "                        provider_id=provider\n",
    "                    )\n",
    "                    \n",
    "                    final_db_id = f\"{vector_db_id}_{provider}_{model}\"\n",
    "                    print(f\"‚úÖ Vector DB created with LLM: {final_db_id}\")\n",
    "                    return final_db_id\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå LLM embedding failed {provider}/{model}: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def create_rag_agent_with_working_config(client, vector_db_id):\n",
    "    \"\"\"Create an agent with RAG using the working vector DB.\"\"\"\n",
    "    print(f\"\\nü§ñ Creating RAG Agent with Vector DB: {vector_db_id}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if not vector_db_id:\n",
    "        print(\"‚ùå No working vector DB available\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # First, try to add some documents to the vector DB\n",
    "        print(\"üìÑ Adding documents to vector DB...\")\n",
    "        \n",
    "        documents = [\n",
    "            RAGDocument(\n",
    "                document_id=\"openshift-guide\",\n",
    "                content=\"https://www.openshift.guide/openshift-guide-screen.pdf\",\n",
    "                mime_type=\"application/pdf\",\n",
    "                metadata={\"source\": \"openshift-guide\"},\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Try to insert documents\n",
    "        # Note: We might need to adjust this based on the actual API\n",
    "        try:\n",
    "            if hasattr(client, 'tool_runtime') and hasattr(client.tool_runtime, 'rag_tool'):\n",
    "                client.tool_runtime.rag_tool.insert(\n",
    "                    documents=documents,\n",
    "                    vector_db_id=vector_db_id,\n",
    "                    chunk_size_in_tokens=512\n",
    "                )\n",
    "                print(\"‚úÖ Documents inserted via tool_runtime\")\n",
    "            else:\n",
    "                # Try alternative approach\n",
    "                print(\"‚ùå tool_runtime.rag_tool not available, trying direct insertion...\")\n",
    "                # We might need to implement this differently\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Document insertion failed: {e}\")\n",
    "            print(\"üîÑ Continuing with agent creation anyway...\")\n",
    "        \n",
    "        # Create agent with RAG tool\n",
    "        agent_config = {\n",
    "            \"model\": \"llama32-3b\",\n",
    "            \"instructions\": \"\"\"You are a helpful assistant with access to OpenShift documentation through RAG.\n",
    "            Use the RAG tool to search for relevant information when answering questions about OpenShift, Kubernetes, or container technologies.\n",
    "            Always search the documentation first before providing answers.\"\"\",\n",
    "            \"tools\": [\n",
    "                {\n",
    "                    \"name\": \"builtin::rag\",\n",
    "                    \"args\": {\n",
    "                        \"vector_db_ids\": [vector_db_id]\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"sampling_params\": {\n",
    "                \"strategy\": {\"type\": \"greedy\"},\n",
    "                \"max_tokens\": 512,\n",
    "                \"temperature\": 0.1\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        agent_response = client.agents.create(agent_config=agent_config)\n",
    "        agent_id = agent_response.agent_id\n",
    "        print(f\"‚úÖ RAG Agent created successfully: {agent_id}\")\n",
    "        return agent_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå RAG Agent creation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_response_content(turn_response):\n",
    "    \"\"\"Extract content from the streaming response properly.\"\"\"\n",
    "    print(\"üîÑ Processing streaming response...\")\n",
    "    \n",
    "    response_content = \"\"\n",
    "    chunk_count = 0\n",
    "    \n",
    "    for chunk in turn_response:\n",
    "        chunk_count += 1\n",
    "        \n",
    "        # Look for text in the event payload\n",
    "        if hasattr(chunk, 'event') and chunk.event:\n",
    "            event = chunk.event\n",
    "            if hasattr(event, 'payload'):\n",
    "                payload = event.payload\n",
    "                \n",
    "                # Check for text delta (this is where the actual content is)\n",
    "                if hasattr(payload, 'delta') and hasattr(payload.delta, 'text'):\n",
    "                    text = payload.delta.text\n",
    "                    response_content += text\n",
    "                    print(f\"  üìù Added: '{text}'\")\n",
    "                \n",
    "                # Check for complete step content\n",
    "                elif hasattr(payload, 'step_details') and hasattr(payload.step_details, 'api_model_response'):\n",
    "                    content = payload.step_details.api_model_response.content\n",
    "                    if content and content not in response_content:\n",
    "                        response_content = content  # Use the complete content\n",
    "                        print(f\"  üìã Complete content: '{content}'\")\n",
    "    \n",
    "    print(f\"‚úÖ Extracted {len(response_content)} characters from {chunk_count} chunks\")\n",
    "    return response_content\n",
    "\n",
    "\n",
    "def test_rag_agent(client, agent_id):\n",
    "    \"\"\"Test the RAG agent with OpenShift questions.\"\"\"\n",
    "    print(f\"\\nüß™ Testing RAG Agent: {agent_id}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if not agent_id:\n",
    "        print(\"‚ùå No RAG agent to test\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Create session\n",
    "        session_response = client.agents.session.create(\n",
    "            agent_id=agent_id,\n",
    "            session_name=f\"rag-test-{int(time.time())}\"\n",
    "        )\n",
    "        session_id = session_response.session_id\n",
    "        print(f\"‚úÖ Session created: {session_id}\")\n",
    "        \n",
    "        # Test questions\n",
    "        test_questions = [\n",
    "            \"What is OpenShift?\",\n",
    "            \"How do I install OpenShift?\",\n",
    "            \"What are the main differences between OpenShift and Kubernetes?\"\n",
    "        ]\n",
    "        \n",
    "        for i, question in enumerate(test_questions, 1):\n",
    "            print(f\"\\nüìã Question {i}: {question}\")\n",
    "            \n",
    "            turn_response = client.agents.turn.create(\n",
    "                agent_id=agent_id,\n",
    "                session_id=session_id,\n",
    "                messages=[{\"role\": \"user\", \"content\": question}],\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            response_content = extract_response_content(turn_response)\n",
    "            print(f\"ü§ñ Response: {response_content}\")\n",
    "            \n",
    "            if not response_content:\n",
    "                print(\"‚ùå No response content received\")\n",
    "            else:\n",
    "                print(f\"‚úÖ Response received ({len(response_content)} chars)\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå RAG agent test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "def comprehensive_rag_setup():\n",
    "    \"\"\"Complete RAG setup with discovery.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üîß COMPREHENSIVE RAG SETUP WITH DISCOVERY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create client\n",
    "    client = LlamaStackClient(base_url=os.getenv(\"REMOTE_BASE_URL\", \"http://llamastack-server:8321\"))\n",
    "    \n",
    "    # Step 1: Discover providers\n",
    "    providers = discover_vector_db_providers(client)\n",
    "    print(f\"\\n‚úÖ Working providers: {providers}\")\n",
    "    \n",
    "    # Step 2: Discover embedding models\n",
    "    embedding_models = discover_embedding_models(client)\n",
    "    print(f\"‚úÖ Embedding models: {embedding_models}\")\n",
    "    \n",
    "    # Step 3: Create working vector DB\n",
    "    vector_db_id = test_vector_db_with_discovered_info(client, providers, embedding_models)\n",
    "    print(f\"‚úÖ Vector DB: {vector_db_id}\")\n",
    "    \n",
    "    # Step 4: Create RAG agent\n",
    "    rag_agent_id = create_rag_agent_with_working_config(client, vector_db_id)\n",
    "    print(f\"‚úÖ RAG Agent: {rag_agent_id}\")\n",
    "    \n",
    "    # Step 5: Test RAG agent\n",
    "    if rag_agent_id:\n",
    "        test_success = test_rag_agent(client, rag_agent_id)\n",
    "        print(f\"‚úÖ RAG Test: {'Passed' if test_success else 'Failed'}\")\n",
    "    \n",
    "    return client, rag_agent_id, vector_db_id\n",
    "\n",
    "\n",
    "# Run the comprehensive setup\n",
    "if __name__ == \"__main__\":\n",
    "    client, rag_agent_id, vector_db_id = comprehensive_rag_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
